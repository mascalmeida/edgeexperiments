{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages, seed and path\n",
    "## packages\n",
    "from edge_sim_py import *\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import msgpack\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## seed\n",
    "import torch\n",
    "torch.manual_seed(5)\n",
    "import random\n",
    "random.seed(5)\n",
    "import numpy as np\n",
    "np.random.seed(5)\n",
    "\n",
    "## path\n",
    "algo_name = \"worstfit_lm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "# def custom_collect_method(self) -> dict: # Custom collect method to measure the power consumption of each server\n",
    "#     metrics = {\n",
    "#         \"Instance ID\": self.id,\n",
    "#         \"Power Consumption\": self.get_power_consumption(),\n",
    "#     }\n",
    "#     return metrics\n",
    "\n",
    "power_list = list() # List to store total power consumption everytime the task scheduling algorithm is used\n",
    "\n",
    "\n",
    "def my_algorithm(parameters):\n",
    "    \n",
    "    print(\"\\n\\n\")\n",
    "    total_power = 0 #We sum the power consumption after migrating each service\n",
    "    for service in Service.all(): #Iterate over every service\n",
    "        \n",
    " \n",
    "        if not service.being_provisioned: #If service needs to be migrated\n",
    "\n",
    "            \n",
    "            #To sort edge servers based on their available free resources, we will use Python's \"sorted\" method. \n",
    "            #The server capacity is represented by three layers: CPU, memory, and disk. To determine the average resource utilization of each server, \n",
    "            #we calculate the geometric mean of these three layers. Finally, we set the \"reverse\" attribute of the sorted method to \"True,\" \n",
    "            #allowing us to arrange the edge servers in descending order of their free resources.\n",
    "\n",
    "\n",
    "            edge_servers = sorted(\n",
    "                EdgeServer.all(),\n",
    "                key=lambda s: ((s.cpu - s.cpu_demand) * (s.memory - s.memory_demand) * (s.disk - s.disk_demand)) ** (1 / 3),\n",
    "                reverse=True,\n",
    "            )\n",
    "\n",
    "            for edge_server in edge_servers:\n",
    "                # Check if the edge server has resources to host the service\n",
    "                if edge_server.has_capacity_to_host(service=service):\n",
    "                    # We just need to migrate the service if it's not already in the least occupied edge server\n",
    "                    if service.server != edge_server:\n",
    "                        print(f\"[STEP {parameters['current_step']}] Migrating {service} From {service.server} to {edge_server}\")\n",
    "                        \n",
    "                        service.provision(target_server=edge_server)\n",
    "\n",
    "                        \n",
    "                        #get the sum of powerconsumption of each edge server\n",
    "                        power = 0\n",
    "\n",
    "                        for iter_edge_server in EdgeServer.all():\n",
    "                            power = power + iter_edge_server.get_power_consumption()\n",
    "                        \n",
    "                        #Add power consumption after migrating current service\n",
    "                        total_power += power\n",
    "                        \n",
    "                        # After start migrating the service we can move on to the next service\n",
    "                        break\n",
    "\n",
    "    #Append to power_list for plotting\n",
    "    power_list.append(total_power)\n",
    "\n",
    "\n",
    "def stopping_criterion(model: object):    \n",
    "    # As EdgeSimPy will halt the simulation whenever this function returns True,\n",
    "    # its output will be a boolean expression that checks if the current time step is 600\n",
    "    return model.schedule.steps == 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation execution\n",
    "simulator = Simulator(\n",
    "    tick_duration=1,\n",
    "    tick_unit=\"seconds\",\n",
    "    stopping_criterion=stopping_criterion,\n",
    "    resource_management_algorithm=my_algorithm,\n",
    ")\n",
    "\n",
    "# Loading a sample dataset\n",
    "#simulator.initialize(input_file=\"sample_dataset3.json\")\n",
    "simulator.initialize(input_file=\"https://raw.githubusercontent.com/EdgeSimPy/edgesimpy-tutorials/master/datasets/sample_dataset2.json\")\n",
    "\n",
    "#Assigning the custom collect method\n",
    "#EdgeServer.collect = custom_collect_method\n",
    "\n",
    "# Executing the simulation\n",
    "simulator.run_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results\n",
    "## Retrieving logs dataframe for plot\n",
    "logs_containerregistry = pd.DataFrame(simulator.agent_metrics[\"ContainerRegistry\"])\n",
    "logs_edgeserver = pd.DataFrame(simulator.agent_metrics[\"EdgeServer\"])\n",
    "logs_networkflow = pd.DataFrame(simulator.agent_metrics[\"NetworkFlow\"])\n",
    "logs_networkswitch = pd.DataFrame(simulator.agent_metrics[\"NetworkSwitch\"])\n",
    "logs_service = pd.DataFrame(simulator.agent_metrics[\"Service\"])\n",
    "logs_user = pd.DataFrame(simulator.agent_metrics[\"User\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_edgeserver['CPU Usage'] = (logs_edgeserver['CPU Demand']*100)/logs_edgeserver['CPU']\n",
    "logs_edgeserver['RAM Usage'] = (logs_edgeserver['RAM Demand']*100)/logs_edgeserver['RAM']\n",
    "logs_edgeserver[['Object', 'Power Consumption', 'CPU Usage', 'RAM Usage']].groupby(by=['Object']).mean()#.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_edgeserver"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edgeai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
